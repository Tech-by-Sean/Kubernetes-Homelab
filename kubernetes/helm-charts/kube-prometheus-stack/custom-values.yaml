# Disable Talos control plane monitoring (not exposed on standard ports)
kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false
kubeEtcd:
  enabled: false
kubeProxy:
  enabled: false

# Alertmanager configuration for Telegram
alertmanager:
  enabled: true
  alertmanagerSpec:
    secrets:
      - alertmanager-telegram
  
  config:
    global:
      resolve_timeout: 5m
    
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'telegram'
      routes:
      - receiver: 'null'
        matchers:
        - alertname =~ "InfoInhibitor|Watchdog"
    
    receivers:
    - name: 'null'  # Add null receiver for default routes
    - name: 'telegram'
      telegram_configs:
      - bot_token_file: /etc/alertmanager/secrets/alertmanager-telegram/bot-token
        chat_id: 768244878
        parse_mode: 'HTML'
        message: |
          {{ range .Alerts }}
          <b>{{ .Labels.alertname }}</b>
          Status: {{ .Status }}
          {{ range .Labels.SortedPairs }}â€¢ {{ .Name }}: {{ .Value }}
          {{ end }}
          {{ .Annotations.description }}
          {{ end }}

# Enable Prometheus with persistent storage
prometheus:
  prometheusSpec:
    retention: 30d
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi

# Enable Grafana with existing PVC
grafana:
  enabled: true
  persistence:
    enabled: true
    existingClaim: grafana-pvc
  
  # Grafana admin credentials
  adminPassword: YourSecurePasswordHere

additionalPrometheusRulesMap:
  pod-monitoring:
    groups:
    - name: pod-health
      interval: 30s
      rules:
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[5m]) > 0
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} (container {{ $labels.container }}) has restarted in the last 5 minutes"
      
      - alert: PodNotReady
        expr: kube_pod_status_phase{phase!~"Running|Succeeded"} > 0
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in {{ $labels.phase }} phase for more than 30 seconds"
      
      - alert: PodOOMKilled
        expr: kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} > 0
        for: 15s
        labels:
          severity: critical
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} was OOM killed"
          description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} was killed due to out of memory"
      
      - alert: DeploymentReplicasMismatch
        expr: kube_deployment_spec_replicas{namespace!~"kube-.*"} != kube_deployment_status_replicas_available{namespace!~"kube-.*"}
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has replica mismatch"
          description: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} wants {{ $labels.spec_replicas }} replicas but only has {{ $labels.available_replicas }} available"
      
      - alert: StatefulSetReplicasMismatch
        expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has replica mismatch"
          description: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has {{ $labels.ready_replicas }} ready but wants {{ $labels.replicas }}"
      
      - alert: DaemonSetNotScheduled
        expr: kube_daemonset_status_desired_number_scheduled != kube_daemonset_status_current_number_scheduled
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} is not fully scheduled"
          description: "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} wants {{ $labels.desired_number_scheduled }} pods but only has {{ $labels.current_number_scheduled }} scheduled"
      
      - alert: PodPending
        expr: kube_pod_status_phase{phase="Pending"} > 0
        for: 45s
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} stuck in Pending"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in Pending state for more than 45 seconds. Check node resources and pod events."